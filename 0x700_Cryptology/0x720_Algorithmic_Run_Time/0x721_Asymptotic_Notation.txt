Asymptotic notation is a way to express an algorithm's efficiency
It's called asymptotic because it deals with the behavior of the algorithm as the input size approaches the asymptotic limit of infinity

Returning to the examples of the 2n+365 algorithm and the 2n^2+5 algorithm, we determined that the 2n+365 algorithm is generally more efficient because it follows the trend of n, while the 2n^2+5 algorithm follows the general trend of n^2
This means that 2n+365 is bounded above by a positive multiple of n for all sufficiently large n, and 2n^2+5 is bounded above by a positive multiple of n^2 for all sufficiently large n

This sounds kind of confusing, but all it really means is that there exists a positive constant for the trend value and a lower bound on n, such that the trend value multiplied by the constant will always be greater than the time complexity for all n greater than the lower bound
In other words, 2n^2+5 is in the order of n^2, and 2n+365 is in the order of n
There's a convenient mathematical notation for this, called big-oh notation, which looks like O(n^2) to describe an algorithm that is in the order of n^2

A simple way to convert an algorithm's time complexity to big-oh notation is to simply look at the high-order terms, since these will be the terms that matter most as n becomes sufficiently large
So an algorithm with a time complexity of 3n^4+43n^3+763n+logn+37 would be in the order of O(n^4) and 54n^7+23n^4+4325 would be O(n^7)